q()
heloo
hello
x<-c(2,3,4)
delete x11()
x
c
x
define x
define(x)
x=0
x
x=nullfile()
x
c
a<-c(2,3,)
a<-c(2,3,,)
a<-c(2,3,4)
b<-c(1,2)
d<-a*b
d
pwd
pwd()
wd
pwd
pwd()
wd
wd()
getwd
getwd()
getwd()
install.packages("ggplot2","numpy","seaborn")
install.packages("PANDA")
"""
Web Scraping - Beautiful Soup
"""
# importing required libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
# target URL to scrap
url = "https://www.goibibo.com/hotels/hotels-in-shimla-ct/"
# headers
headers = {
'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
}
# send request to download the data
response = requests.request("GET", url, headers=headers)
# parse the downloaded data
data = BeautifulSoup(response.text, 'html.parser')
print(data)
reticulate::repl_python()
y
"""
Web Scraping - Beautiful Soup
"""
# importing required libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
# target URL to scrap
url = "https://www.goibibo.com/hotels/hotels-in-shimla-ct/"
# headers
headers = {
'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
}
# send request to download the data
response = requests.request("GET", url, headers=headers)
# parse the downloaded data
data = BeautifulSoup(response.text, 'html.parser')
print(data)
install.packages("beautifulsoup")
quit
install.packages("beautifulsoup")
install.packages("BeautifulSoup")
$ conda activate r-reticulate
#
# To activate this environment, use
#
$ conda activate r-reticulate
#
# To activate this environment, use
#
$ conda activate r-reticulate
$ conda activate r-reticulate
$ conda activate r-reticulate
$ conda activate r-reticulate
reticulate::repl_python()
"""
Web Scraping - Beautiful Soup
"""
$ conda activate r-reticulate
# importing required libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
# target URL to scrap
url = "https://www.goibibo.com/hotels/hotels-in-shimla-ct/"
# headers
headers = {
'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
}
# send request to download the data
response = requests.request("GET", url, headers=headers)
# parse the downloaded data
data = BeautifulSoup(response.text, 'html.parser')
print(data)
--------------------------
"""
Web Scraping - Beautiful Soup
"""
$ conda activate r-reticulate
# importing required libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
# target URL to scrap
url = "https://www.goibibo.com/hotels/hotels-in-shimla-ct/"
# headers
headers = {
'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
}
# send request to download the data
response = requests.request("GET", url, headers=headers)
# parse the downloaded data
data = BeautifulSoup(response.text, 'html.parser')
print(data)
quit
install.packages("ggplot2","numpy","bs4")
reticulate::repl_python()
$ conda activate r-reticulate
install.packages("BeautifulSoup")
reticulate::repl_python()
$ conda deactivate
install.packages("ggplot2","numpy","bs4")
install.packages('numpy')
install.packages("BeautifulSoup")
install.packages('bs4')
reticulate::repl_python()
"""
Web Scraping - Beautiful Soup
"""
$ conda activate r-reticulate
#$ conda deactivate
# importing required libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
# target URL to scrap
url = "https://www.goibibo.com/hotels/hotels-in-shimla-ct/"
# headers
headers = {
'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
}
# send request to download the data
response = requests.request("GET", url, headers=headers)
# parse the downloaded data
data = BeautifulSoup(response.text, 'html.parser')
print(data)
reticulate::repl_python()
"""
Web Scraping - Beautiful Soup
"""
$ conda activate r-reticulate
#$ conda deactivate
# importing required libraries
import requests
from bs4 import BeautifulSoup
import pandas as pd
# target URL to scrap
url = "https://www.goibibo.com/hotels/hotels-in-shimla-ct/"
# headers
headers = {
'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
}
# send request to download the data
response = requests.request("GET", url, headers=headers)
# parse the downloaded data
data = BeautifulSoup(response.text, 'html.parser')
print(data)
# find all the sections with specifiedd class name
cards_data = data.find_all('div', attrs={'class', 'width100 fl htlListSeo hotel-tile-srp-container hotel-tile-srp-container-template new-htl-design-tile-main-block'})
# total number of cards
print('Total Number of Cards Found : ', len(cards_data))
# source code of hotel cards
# for card in cards_data:
#     print(card)
# extract the hotel name and price per room
for card in cards_data:
# get the hotel name
hotel_name = card.find('p')
# get the room price
room_price = card.find('li', attrs={'class': 'htl-tile-discount-prc'})
print(hotel_name.text, room_price.text)
# create a list to store the data
scraped_data = []
for card in cards_data:
# initialize the dictionary
card_details = {}
# get the hotel name
hotel_name = card.find('p')
# get the room price
room_price = card.find('li', attrs={'class': 'htl-tile-discount-prc'})
# add data to the dictionary
card_details['hotel_name'] = hotel_name.text
card_details['room_price'] = room_price.text
# append the scraped data to the list
scraped_data.append(card_details)
#
# # create a data frame from the list of dictionaries
dataFrame = pd.DataFrame.from_dict(scraped_data)
#
# # save the scraped data as CSV file
dataFrame.to_csv('hotels_data.csv', index=False)
library(readr)
hotels_data_Copy1 <- read_csv("C:/Users/enteruser/Desktop/SGA07_DATASCI/hotels_data-Copy1.csv")
View(hotels_data_Copy1)
Print(hotels_data_Copy1)
view(hotels_data_Copy1)
View(hotels_data_Copy1)
View(hotels_data_Copy1)
library(dplr)
install.packages("dplR")
pwd()
wd()
setwd("C:/Users/enteruser/Desktop/SGA07_DATASCI")
all_accounts<-read_csv("raw/all_accounts.csv")
all_balances <- read_csv("raw/all_balances.csv")
all_demographics <- read_delim("raw/all_demographics.csv", "|", escape_double = FALSE, trim_ws = TRUE)
all_investments <- read_csv("raw/all_investments.csv")
all_loans <- read_csv("raw/all_loans.csv")
all_loans_application <- read_csv("raw/all_loans_application.csv")
all_transactions_digital <- read_csv("raw/all_transactions_digital.csv")
library(readr)
all_accounts<-read_csv("raw/all_accounts.csv")
all_balances <- read_csv("raw/all_balances.csv")
all_demographics <- read_delim("raw/all_demographics.csv", "|", escape_double = FALSE, trim_ws = TRUE)
all_investments <- read_csv("raw/all_investments.csv")
all_loans <- read_csv("raw/all_loans.csv")
all_loans_application <- read_csv("raw/all_loans_application.csv")
all_transactions_digital <- read_csv("raw/all_transactions_digital.csv")
bureau_score <- read_csv("raw/bureau_score.csv")
dictionary <- read_csv("raw/dictionary.csv", skip=1)
summary(bureau_score$BUREAU_SCORE)
sum(bureau_score$BUREAU_SCORE)
str(bureau_score$BUREAU_SCORE)
str(bureau_score$All)
str(bureau_score[1]
str(bureau_score[1])
str(bureau_score[2])
str(bureau_score[1])
summary(bureau_score[1])
summary(bureau_score[2])
summary(bureau_score[3])
str(bureau_score[3])
sum(bureau_score[3])
sum(bureau_score[3]==NA)
count.fields(bureau_score[3]==NA)
count(bureau_score[3]==NA)
count_fields(bureau_score[3]==NA)
sum(bureau_score[3]==NA)
str(bureau_score[3])
bureauscore
View(bureauscore    )
remove(bureauscore    )
remove(bureauscore)
bureau_score
is.dataframe(bureau_score)
is.data.frame(bureau_score)
names(bureau_score)
variable.names(bureau_score)
library(readr,dply)
library(readr,dply,ggplot2)
variable.names(bureau_score)
View(all_accounts)
all_accounts <- all_accounts %>% drop=c('MASKED ACCOUNT')
all_accounts[-1]
all_accounts[-c(1,2)]
all_accounts[-c(all_accounts$MASKED_ACCOUNT)]
all_accounts[-all_accounts$MASKED_ACCOUNT]
all_accounts[-2]
unique(all_accounts[1])
sum(unique(all_accounts[1]))
count.fields(unique(all_accounts[1]))
length(unique(all_accounts[1]))
sum(unique(all_accounts[1]))
nrow(unique(all_accounts[1]))
(unique(all_accounts[1]))
row(unique(all_accounts[1]))
row(unique(all_accounts[2]))
nrow(unique(all_accounts[1]))
nrow((all_accounts[1]))
unique
unique
unique
nrows(unique(all_accounts[1]))
nrow(unique(all_accounts[1]))
all_accounts$RECORD_STAT
all_accounts$RECORD_STAT="C"
all_accounts$RECORD_STAT
all_accounts<-read_csv("raw/all_accounts.csv")
all_accounts$RECORD_STAT=="C"
sum(all_accounts$RECORD_STAT=="C")
length(all_accounts$RECORD_STAT=="C")
all_accounts$ACY_AVL_BAL(all_accounts$RECORD_STAT=="C")
all_accounts$ACY_AVL_BAL[all_accounts$RECORD_STAT=="C"]
sum(all_accounts$ACY_AVL_BAL[all_accounts$RECORD_STAT=="C"]>0)
sum(all_accounts$ACY_AVL_BAL[all_accounts$RECORD_STAT=="C"]>0)
library(data.table)
library(readr,dply,ggplot2)
library(readr,dply,ggplot2,tidyverse)
install.packages("tidyverse")
library(tidyselect)
library(tidyr)
all_accounts == all_accounts %>% group_by(CUSTOMER_UNIQUE_ID)
all_accounts
all_accounts <- all_accounts %>% group_by(CUSTOMER_UNIQUE_ID)
install.packages("knitr")
library(lubridate)
library(readr,dply,ggplot2,tidyverse,lubridate)
all_accounts <- all_accounts %>% group_by(CUSTOMER_UNIQUE_ID)
library(readr,dply,ggplot2,tidyverse,lubridate)
library(tidyverse)
library(readr,dply,ggplot2,tidyverse,lubridate)
library(readr,dply,ggplot2,tidyverse,lubridate)
